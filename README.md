**Matiks \<\> AIMS-DTU Ideathon**  
**Idea Submission by Mayank Jangid**

**Introduction**  
This proposal outlines a solution for an AI-powered generator designed to create diverse math problems and puzzles aimed at competitive learning. The system will use generative AI and algorithmic methods to generate challenging and varied problems, encouraging quick thinking and problem-solving skills relevant to competitive exams.

**Objective**  
The goal is to develop a platform that dynamically generates randomized and unique math problems across categories such as sequences, number patterns, logic puzzles, and other aptitude-related challenges. These problems will be solvable within 5-60 seconds, supporting rapid cognitive engagement and skill enhancement.

**Idea**  
Our motive is to use generative AI for problem generation, in this case I have used open source LLMs for implementation of my ideas such as **llama3.1** and **mathstral** (fine tuned model for mathematical solving based on llama framework).

*The sample demonstration code is available on my github repo: [https://github.com/mayank-jangid-moon/matiks-ideathon-LLM](https://github.com/mayank-jangid-moon/matiks-ideathon-LLM)*

The main ideology I have followed is to prompt our LLM with all the basic rules and regulations it has to follow, and then provide it with some data it can learn from and then generate more new and unique problems inspired from that format. For instance let us take examples of series completion questions. LLMs are bad with math and they don't exactly know how to solve a question so it is necessary to show them how we think through a question and how we solve a problem. This approach is also known as **Chain of Thought** prompting in which our LLM has to think at each and every step and then execute it. In this case I gave a sample of 20 series questions to the LLM along with the complete stepwise solution to it. I had strictly denied the LLM to perform certain actions , such as revealing the formula or answer to the question while generating.   

Our LLM successfully generated the number of questions we instructed it to generate. Now the next step is to check for their answers and check if the generated question is even solvable or is just some crap generated by the LLM.

For that i had set up another LLM, the mathstral LLM is already fine tuned for problem solving thus i did not have to prompt it regarding how to solve a question. I have the directions on how to approach a question and not to give a forceful solution to every question, rather to point out some questions which don't have a proper solution as wrong so that they can be dropped.   

*(as you can see our model was even able to point out one question (no. 4\) as incorrect as it did not have any possible answer)*  
This approach of using multiple models to interact with each other and then give a final output is what makes an **AI Agent.**  
In this case we are using one model to generate the questions and second model to generate the answers and validate the questions. We can even separate the answer generation and validation step too to increase the accuracy.  
We are using a different model for generation and validation to prevent our model from hallucinating, because the model which has generated the question is highly probable to hallucinate and assume something to solve the question which may be wrong.

The model I have used in this case (llama3.1:8B) is a pretty small one because of technical limitations. We can use models like llama3.1:70B for even better accuracy.

In such a way we can train different LLMs specializing in generation and validation of different types of problems by providing them a context with sample problems along with the chain of thoughts to solve them.

Now for the part of categorizing a question as easy, medium and difficult, we can include a parameter of the type of question we want to generate through our LLM. To successfully execute that we would need to lay down some rules for each type of question like the type of calculations they can include and the amount of solving time they should require. We would also need to provide the sample of questions in prompt in a categorical format now, that is classifying each sample question as easy, medium and hard so that our LLM can learn which kind of questions belong to each category and then generate new questions for different categories.

**Bonus Implementation:**  
To implement a feature where two users can compete head-to-head, questions for both of them should be generated through a common LLM. The questions generated can be stored in a **buffer/pipeline** and can be shown to each one of them as they keep solving it. The scoring can be based on a formula which incorporates the time it took to solve (more the time less the score), the amount of wrong attempts (more the attempts, less the score), and the difficulty of the question (more the difficulty more the score). The main focus in this case would be that each person gets the same set of questions to be fair.

As far as dynamic integration is concerned, we can change the difficulty of question generation from easy to medium to hard gradually as the person keeps on solving them. If the person struggles to solve the questions then we can gradually decrease the difficulty of the question. To improve this rather than having 3 categories, we can have the **difficulty of the question on a scale of 1-10**, so that the transition in the difficulty can be gradual and not abrupt.

